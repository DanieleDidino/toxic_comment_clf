{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniele-didino\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"train.csv\"))\n",
    "val_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"val.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters & wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 1 # 20\n",
    "MAX_LEN = 20\n",
    "EMBED_DIM = 50\n",
    "NUM_CLASSES = 6 # toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hyperparams():\n",
    "    return {\n",
    "        \"embedding_dim\": random.choice([50, 100, 150]),\n",
    "        \"hidden_units\": random.choice([64, 128, 256]),\n",
    "        \"num_layers\": random.randint(1, 3),\n",
    "        \"dropout\": random.uniform(0.2, 0.5),\n",
    "        \"learning_rate\": 10 ** random.uniform(-4, -2),\n",
    "        \"batch_size\": random.choice([16, 32, 64]),\n",
    "        \"num_classes\": 6, # toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "        \"epochs\": 5 # random.randint(2, 5)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (5.4s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniele/Desktop/Projects/toxic_comment_clf/wandb/run-20250224_145229-xyp6zo47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/xyp6zo47' target=\"_blank\">glowing-night-1</a></strong> to <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/xyp6zo47' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/runs/xyp6zo47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"toxic_comment_clf\",\n",
    "    # Track hyperparameters\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"embed_dim\": EMBED_DIM\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tokenizer and util functions\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_vocab(texts: list[str], min_freq: int=1) -> dict:\n",
    "    token_counts = Counter()\n",
    "    for text in texts:\n",
    "        cleaned_text = clean_text(text)\n",
    "        token_counts.update(cleaned_text.split())\n",
    "    vocab = {word: idx + 2 for idx, (word, count) in enumerate(token_counts.items()) if count >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokenizer(text: str, vocab: dict, max_len: int) -> dict:\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = [vocab.get(word, 1) for word in cleaned_text.split()[:max_len]]\n",
    "    input_ids = tokens + [0] * (max_len - len(tokens))\n",
    "\n",
    "    # Check if token exceeds the len of the voceb\n",
    "    for token in input_ids:\n",
    "        if token >= len(vocab):\n",
    "            print(f\"Warning: Token index {token} out of range!\")\n",
    "    \n",
    "    return {'input_ids': torch.tensor(input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your recent edits, something to read, and a point of view \n",
      "\n",
      "Hi, please take the time to read Wikipedia:Guidance for younger editors when you have a moment. Please also be aware that it not only applies to things you post on Wikipedia, but also to things you ask others on Wikipedia.\n",
      "\n",
      "Secondly, there is no minimum age to edit Wikipedia, and it certainly doesn't just happen to coincide conveniently with however old you happen to be today. Some 15 year olds are administrators, some people have been administrators and bureaucrats while aged 12, some 16 year olds and 64 year olds are banned from Wikipedia by the community. Actions, not numbers, are an indication of maturity.  (talk)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.comment_text[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your recent edits something to read and a point of view \n",
      "\n",
      "hi please take the time to read wikipediaguidance for younger editors when you have a moment please also be aware that it not only applies to things you post on wikipedia but also to things you ask others on wikipedia\n",
      "\n",
      "secondly there is no minimum age to edit wikipedia and it certainly doesnt just happen to coincide conveniently with however old you happen to be today some 15 year olds are administrators some people have been administrators and bureaucrats while aged 12 some 16 year olds and 64 year olds are banned from wikipedia by the community actions not numbers are an indication of maturity  talk\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(train_data.comment_text[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tmp = build_vocab(train_data.comment_text.to_list(), MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and - 2\n",
      "that - 3\n",
      "would - 4\n",
      "verify - 5\n",
      "john - 6\n",
      "was - 7\n",
      "a - 8\n",
      "pratt - 9\n",
      "grad - 10\n",
      "w - 11\n",
      "babs - 12\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for k,v in vocab_tmp.items():\n",
    "    print(f\"{k} - {v}\")\n",
    "    c += 1\n",
    "    if c > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.  And that would verify that John was a Pratt grad w/ BA/BS in Graphic Art'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.comment_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2,  3,  4,  5,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_data.comment_text[0], vocab_tmp, max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2,  3,  4,  5,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"and that would verify that john was a pratt grad w babs in graphic art'\", vocab_tmp, max_len=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dummy(df: pd.DataFrame) -> dict:\n",
    "    dummy_pred = pd.DataFrame(\n",
    "        0,\n",
    "        index=df.index,\n",
    "        columns=df.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].columns\n",
    "    )\n",
    "\n",
    "    df_labels = df.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.flatten()\n",
    "    dummy_pred = dummy_pred.values.flatten()\n",
    "\n",
    "    accuracy = accuracy_score(df_labels, dummy_pred)\n",
    "    precision = precision_score(df_labels, dummy_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(df_labels, dummy_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(df_labels, dummy_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # AUC-ROC (for multi-label, compute per class and take average)\n",
    "    auc_roc = roc_auc_score(df_labels, dummy_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_roc': auc_roc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9636\n",
      "Precision: 0.4818\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.4907\n",
      "AUC-ROC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "train_metrics_dummy = evaluate_dummy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9626\n",
      "Precision: 0.4813\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.4905\n",
      "AUC-ROC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "val_metrics_dummy = evaluate_dummy(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **accuracy** is very high (~0.96),\n",
    "an **AUC-ROC** around 0.5 indicates that this approach is equivalent to random guessing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        encoded = self.tokenizer(text)\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "\n",
    "# Model\n",
    "class ToxicityClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_units, num_layers, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        layers = []\n",
    "        input_size = embed_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(input_size, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = hidden_units\n",
    "        layers.append(nn.Linear(hidden_units, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids).mean(dim=1)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "# Compute Loss and Metrics\n",
    "def model_eval(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients during evaluation\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to convert logits to probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            # Save predictions\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append((probs >= threshold).int().cpu())\n",
    "\n",
    "    # Concatenate results\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # AUC-ROC (for multi-label, compute per class and take average)\n",
    "    auc_roc = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "\n",
    "    return avg_loss, auc_roc\n",
    "\n",
    "\n",
    "# Training function\n",
    "def model_train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # set model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        _, train_auc_roc = model_eval(model, train_loader, criterion, device)\n",
    "        val_loss, val_auc_roc = model_eval(model, val_loader, criterion, device)\n",
    "        \n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | AUC_ROC: {train_auc_roc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | AUC_ROC: {val_auc_roc:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_auc_roc\": train_auc_roc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_auc_roc\": val_auc_roc,\n",
    "        })\n",
    "\n",
    "    return val_auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_data.comment_text.to_list()\n",
    "train_labels = train_data.loc[:, [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "val_input = val_data.comment_text.to_list()\n",
    "val_labels = val_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "vocab = build_vocab(train_input, MIN_FREQ)\n",
    "\n",
    "# Prepare train dataset\n",
    "train_dataset = ToxicCommentsDataset(train_input, train_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)\n",
    "\n",
    "# Prepare validation dataset\n",
    "val_dataset = ToxicCommentsDataset(val_input, val_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.  And that would verify that John was a Pratt grad w/ BA/BS in Graphic Art',\n",
       " 'hi how are you  are you Mr bill \\n\\naoa \\n       hi i am waseem 4rm pakistan n whats a maining of the The International Awareness\\nPromotion Department Of\\nE.A.A.S Lottery Headquarters\\nEuro-Afro Asia Sweepstake lottery he says congratulations you have won US$250,000.00 ( (Two hundred and Fifty Thousand United States Dollars) in Cheque. pl z i have no idea tell me by this number 00923236916674 00923147007006  pless  pless pless  i shell b thank full to you',\n",
       " 'Abi 17:45, 9 February 2014',\n",
       " 'We can agree on one thing: the numbers do speak for themselves. The fact that they seem to be saying something else to you than to me, and some sources, is irrelevant.',\n",
       " 'I noticed that on the media page there are only FM radio stations.  Can someone add some AM stations?']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time required to train 1 epoch:\n",
    "- CPU: ~ 30 minutes\n",
    "- GPU: ~ 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniele/Desktop/Projects/toxic_comment_clf/wandb/run-20250224_162234-mvqw8kt5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/mvqw8kt5' target=\"_blank\">fragrant-darkness-5</a></strong> to <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/mvqw8kt5' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/runs/mvqw8kt5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0832 | AUC_ROC: 0.9765\n",
      "Val Loss: 0.0682 | AUC_ROC: 0.9530\n",
      "Epoch 2/5\n",
      "Train Loss: 0.0560 | AUC_ROC: 0.9885\n",
      "Val Loss: 0.0672 | AUC_ROC: 0.9541\n",
      "Epoch 3/5\n",
      "Train Loss: 0.0447 | AUC_ROC: 0.9936\n",
      "Val Loss: 0.0754 | AUC_ROC: 0.9511\n",
      "Epoch 4/5\n",
      "Train Loss: 0.0366 | AUC_ROC: 0.9965\n",
      "Val Loss: 0.0866 | AUC_ROC: 0.9448\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0297 | AUC_ROC: 0.9978\n",
      "Val Loss: 0.1037 | AUC_ROC: 0.9398\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_auc_roc</td><td>▁▅▇██</td></tr><tr><td>train_loss</td><td>█▄▃▂▁</td></tr><tr><td>val_auc_roc</td><td>▇█▇▃▁</td></tr><tr><td>val_loss</td><td>▁▁▃▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_auc_roc</td><td>0.99779</td></tr><tr><td>train_loss</td><td>0.02968</td></tr><tr><td>val_auc_roc</td><td>0.93978</td></tr><tr><td>val_loss</td><td>0.10366</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-darkness-5</strong> at: <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/mvqw8kt5' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/runs/mvqw8kt5</a><br> View project at: <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250224_162234-mvqw8kt5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: {'embedding_dim': 100, 'hidden_units': 256, 'num_layers': 1, 'dropout': 0.3234595026691648, 'learning_rate': 0.005527264579039489, 'batch_size': 64, 'num_classes': 6, 'epochs': 5} with Accuracy: 0.9398\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for trial in range(1):  # Run these searches\n",
    "    wandb.init(project=\"toxic_comment_clf\", reinit=True)\n",
    "\n",
    "    # Sample hyperparameters randomly\n",
    "    config = random_hyperparams()\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    # Initialize DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ToxicityClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=config[\"embedding_dim\"],\n",
    "        hidden_units=config[\"hidden_units\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        num_classes=config[\"num_classes\"])\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    # Training loop\n",
    "    val_auc_roc = model_train(model, train_dataloader, val_dataloader, criterion, optimizer, config[\"epochs\"], DEVICE)\n",
    "\n",
    "    # Store best model\n",
    "    if val_auc_roc > best_score:\n",
    "        best_score = val_auc_roc\n",
    "        best_params = config\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"Best Model: {best_params} with AUC ROC: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, vocab, tokenizer, max_len, device, threshold=0.5):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Preprocess the input\n",
    "    encoded_input = tokenizer(text, vocab, max_len)\n",
    "    input_ids = encoded_input['input_ids'].unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        logits = model(input_ids)  # model outputs\n",
    "        probabilities = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "    \n",
    "    # Decision threshold\n",
    "    predictions = (probabilities >= threshold).int()\n",
    "    \n",
    "    return probabilities.cpu().numpy(), predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.98331773 0.09205111 0.83193743 0.05810835 0.7263957  0.07695656]\n",
      "Predictions: [1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"You suck and I hate you\"\n",
    "probs, preds = predict(model, test_text, vocab, tokenizer, MAX_LEN, DEVICE)\n",
    "\n",
    "print(f\"Probabilities: {probs[0]}\")\n",
    "print(f\"Predictions: {preds[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [[0.9722405  0.07568419 0.91668546 0.01601476 0.65795    0.04427878]]\n",
      "Predictions: [[1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "probs, preds = predict(model, \"fuck you\", vocab, tokenizer, MAX_LEN, DEVICE)\n",
    "\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Predictions: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train dataset\n",
    "train_input = train_data.comment_text.to_list()\n",
    "train_labels = train_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "train_dataset = ToxicCommentsDataset(train_input, train_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Prepare validation dataset\n",
    "val_input = val_data.comment_text.to_list()\n",
    "val_labels = val_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "val_dataset = ToxicCommentsDataset(val_input, val_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0200\n",
      "AUC-ROC: 0.9978\n"
     ]
    }
   ],
   "source": [
    "avg_loss, auc_roc = model_eval(model, train_dataloader, criterion, DEVICE, threshold=0.5)\n",
    "print(f\"Loss: {avg_loss:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1037\n",
      "AUC-ROC: 0.9398\n"
     ]
    }
   ],
   "source": [
    "avg_loss, auc_roc = model_eval(model, val_dataloader, criterion, DEVICE, threshold=0.5)\n",
    "print(f\"Loss: {avg_loss:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screw you\n",
      "why dont you stick it up your fucking ass than lick it out, block it i dont give a shit you fucking bastard, suck my fucking BALLLLLSSSSSSS!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "text = train_data.comment_text[27]\n",
    "print(text)\n",
    "encoded_input = tokenizer(text, vocab, MAX_LEN)\n",
    "input_ids = encoded_input['input_ids'].unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13.4365, -1.3871,  6.6582, -1.2496,  2.6401, -2.8238]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(input_ids)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9999, 0.4200, 0.9932, 0.0433, 0.9442, 0.1706],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.sigmoid(logits).squeeze(0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1, 0, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "predictions = (probabilities >= 0.5).int()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((predictions, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tox_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

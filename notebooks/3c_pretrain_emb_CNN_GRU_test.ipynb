{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"train.csv\"))\n",
    "val_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"val.csv\"))\n",
    "test_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(filepath, vocab, embedding_dim=100):\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def tokenizer(text, vocab, max_len=150):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
    "    tokens = text.split()\n",
    "    encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens[:max_len]]\n",
    "    encodec_padded = np.pad(encoded, (0, max_len - len(encoded)), constant_values=vocab[\"<PAD>\"])[:max_len]\n",
    "    return torch.tensor(encodec_padded).unsqueeze(0)\n",
    "\n",
    "\n",
    "class ToxicClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embedding_dim, hidden_dim, num_filters, kernel_size, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        # CNN layer\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(50) # This reduces the sequence length\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=num_filters,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        # Fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embedding_dim)\n",
    "        x = x.permute(0, 2, 1) # change shape for conv1d (batch_size, channels, seq_len)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1) # change shape back for GRU (batch_size, seq_len, channels)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.dropout(x[:, -1, :]) # take the last time step\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToxicClassifier(\n",
       "  (embedding): Embedding(184223, 100)\n",
       "  (conv): Conv1d(100, 64, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "  (pool): AdaptiveMaxPool1d(output_size=50)\n",
       "  (gru): GRU(64, 64, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.44, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load config\n",
    "config = json.load(open(\"../model/config.json\"))\n",
    "\n",
    "# Load vocab\n",
    "vocab = json.load(open(\"../model/vocab.json\"))\n",
    "\n",
    "# Load embedding matrix\n",
    "embedding_matrix = load_glove_embeddings(\"../embedding/glove.6B.100d.txt\", vocab, config[\"embedding_dim\"])\n",
    "\n",
    "# Recreate the model with saved hyperparameters\n",
    "model = ToxicClassifier(\n",
    "    embedding_matrix,\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    hidden_dim=config[\"hidden_dim\"],\n",
    "    num_filters=config[\"num_filters\"],\n",
    "    kernel_size=config[\"kernel_size\"],\n",
    "    dropout=config[\"dropout\"],\n",
    "    num_classes=config[\"num_classes\"]\n",
    ")\n",
    "\n",
    "# Using CPU\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"../model/model.pth\"))\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text into numerical sequences\n",
    "def encode_text(text, vocab, max_len=150):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
    "    tokens = text.split()\n",
    "    encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens[:max_len]]\n",
    "    return np.pad(encoded, (0, max_len - len(encoded)), constant_values=vocab[\"<PAD>\"])[:max_len]\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=150):\n",
    "        texts = [encode_text(text, vocab, max_len) for text in texts]\n",
    "        self.texts = [torch.tensor(text, dtype=torch.long) for text in texts]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.texts[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "train_input = train_data.comment_text.to_list()\n",
    "train_labels = train_data.loc[:, [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "val_input = val_data.comment_text.to_list()\n",
    "val_labels = val_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "test_input = test_data.comment_text.to_list()\n",
    "test_labels = test_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "train_dataset = ToxicDataset(train_input, train_labels, vocab, config[\"max_len\"])\n",
    "val_dataset = ToxicDataset(val_input, val_labels, vocab, config[\"max_len\"])\n",
    "test_dataset = ToxicDataset(test_input, test_labels, vocab, config[\"max_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, model):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']#.to(device)\n",
    "            labels = batch['labels']#.to(device)\n",
    "            outputs = model(input_ids)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            all_preds.append(outputs.numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0).T\n",
    "    all_labels = np.concatenate(all_labels, axis=0).T\n",
    "\n",
    "    roc_auc = [roc_auc_score(y_true, y_pred) for y_true, y_pred in zip(all_labels, all_preds)]\n",
    "\n",
    "    return np.mean(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = evaluate_model(train_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score = evaluate_model(val_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = evaluate_model(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set - ROC AUC score: 0.9817554410249442\n",
      "Val set --- ROC AUC score: 0.9767250982210864\n",
      "Test set -- ROC AUC score: 0.9746421629415304\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set - ROC AUC score: {train_score}\")\n",
    "print(f\"Val set --- ROC AUC score: {val_score}\")\n",
    "print(f\"Test set -- ROC AUC score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_test_text = pd.read_csv(Path(\"..\", \"data\", \"kaggle_test\", \"test.csv\"))\n",
    "# Add columns because it's needed for the \"ToxicDataset\" class,\n",
    "# the \"DataLoader\" and \"evaluate_model()\" function\n",
    "labels_columns = pd.DataFrame(\n",
    "    0.5,\n",
    "    index=kaggle_test_text.index,\n",
    "    columns=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "kaggle_test_data = pd.concat([kaggle_test_text, labels_columns], axis=1)\n",
    "\n",
    "kaggle_test_input = kaggle_test_data.comment_text.to_list()\n",
    "kaggle_test_labels = kaggle_test_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "kaggle_test_dataset = ToxicDataset(kaggle_test_input, kaggle_test_labels, vocab, config[\"max_len\"])\n",
    "kaggle_test_dataloader = DataLoader(kaggle_test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in kaggle_test_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        outputs = model(input_ids)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        all_preds.append(outputs.numpy())\n",
    "all_preds = np.concatenate(all_preds, axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle_pred_labels = pd.DataFrame((all_preds.T > 0.5).astype(int))\n",
    "kaggle_pred_labels = pd.DataFrame(all_preds.T)\n",
    "kaggle_test_output = pd.concat([kaggle_test_text, kaggle_pred_labels], axis=1).drop(columns=[\"comment_text\"])\n",
    "kaggle_test_output.columns = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "kaggle_test_output.to_csv(Path(\"..\", \"data\", \"kaggle_test\", \"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, vocab, model):\n",
    "    input = tokenizer(text, vocab, max_len=config[\"max_len\"])\n",
    "    # Make prediction\n",
    "    output = model(input)\n",
    "    prediction = torch.sigmoid(output).detach().numpy()\n",
    "    print([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "    print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "[0.9847456  0.2695209  0.95576817 0.03618464 0.88734657 0.19170015]\n"
     ]
    }
   ],
   "source": [
    "predict(\n",
    "    'screw you\\nwhy dont you stick it up your fucking ass than lick it out, block it i dont give a shit you fucking bastard, suck my fucking BALLLLLSSSSSSS!!!!!!!!!!!!!!!',\n",
    "    vocab,\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "[0.82117516 0.0164214  0.28932157 0.01511631 0.36922097 0.04858212]\n"
     ]
    }
   ],
   "source": [
    "predict(\n",
    "    \"I hate you\",\n",
    "    vocab,\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "[0.21111995 0.00212456 0.02372743 0.00355181 0.04209996 0.0078772 ]\n"
     ]
    }
   ],
   "source": [
    "predict(\n",
    "    \"I love you\",\n",
    "    vocab,\n",
    "    model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tox_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

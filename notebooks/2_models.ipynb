{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniele-didino\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"train.csv\"))\n",
    "val_data = pd.read_csv(Path(\"..\", \"data\", \"processed\", \"val.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters & wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 1 # 20\n",
    "MAX_LEN = 20\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tokenizer and util functions\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_vocab(texts: list[str], min_freq: int=1) -> dict:\n",
    "    token_counts = Counter()\n",
    "    for text in texts:\n",
    "        cleaned_text = clean_text(text)\n",
    "        token_counts.update(cleaned_text.split())\n",
    "    vocab = {word: idx + 2 for idx, (word, count) in enumerate(token_counts.items()) if count >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokenizer(text: str, vocab: dict, max_len: int) -> dict:\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = [vocab.get(word, 1) for word in cleaned_text.split()[:max_len]]\n",
    "    input_ids = tokens + [0] * (max_len - len(tokens))\n",
    "\n",
    "    # Check if token exceeds the len of the voceb\n",
    "    for token in input_ids:\n",
    "        if token >= len(vocab):\n",
    "            print(f\"Warning: Token index {token} out of range!\")\n",
    "    \n",
    "    return {'input_ids': torch.tensor(input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your recent edits, something to read, and a point of view \n",
      "\n",
      "Hi, please take the time to read Wikipedia:Guidance for younger editors when you have a moment. Please also be aware that it not only applies to things you post on Wikipedia, but also to things you ask others on Wikipedia.\n",
      "\n",
      "Secondly, there is no minimum age to edit Wikipedia, and it certainly doesn't just happen to coincide conveniently with however old you happen to be today. Some 15 year olds are administrators, some people have been administrators and bureaucrats while aged 12, some 16 year olds and 64 year olds are banned from Wikipedia by the community. Actions, not numbers, are an indication of maturity.  (talk)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.comment_text[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your recent edits something to read and a point of view \n",
      "\n",
      "hi please take the time to read wikipediaguidance for younger editors when you have a moment please also be aware that it not only applies to things you post on wikipedia but also to things you ask others on wikipedia\n",
      "\n",
      "secondly there is no minimum age to edit wikipedia and it certainly doesnt just happen to coincide conveniently with however old you happen to be today some 15 year olds are administrators some people have been administrators and bureaucrats while aged 12 some 16 year olds and 64 year olds are banned from wikipedia by the community actions not numbers are an indication of maturity  talk\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(train_data.comment_text[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tmp = build_vocab(train_data.comment_text.to_list(), MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and - 2\n",
      "that - 3\n",
      "would - 4\n",
      "verify - 5\n",
      "john - 6\n",
      "was - 7\n",
      "a - 8\n",
      "pratt - 9\n",
      "grad - 10\n",
      "w - 11\n",
      "babs - 12\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for k,v in vocab_tmp.items():\n",
    "    print(f\"{k} - {v}\")\n",
    "    c += 1\n",
    "    if c > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.  And that would verify that John was a Pratt grad w/ BA/BS in Graphic Art'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.comment_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2,  3,  4,  5,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_data.comment_text[0], vocab_tmp, max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2,  3,  4,  5,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"and that would verify that john was a pratt grad w babs in graphic art'\", vocab_tmp, max_len=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dummy(df: pd.DataFrame) -> dict:\n",
    "    dummy_pred = pd.DataFrame(\n",
    "        0,\n",
    "        index=df.index,\n",
    "        columns=df.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].columns\n",
    "    )\n",
    "\n",
    "    df_labels = df.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.flatten()\n",
    "    dummy_pred = dummy_pred.values.flatten()\n",
    "\n",
    "    accuracy = accuracy_score(df_labels, dummy_pred)\n",
    "\n",
    "    # AUC-ROC (for multi-label, compute per class and take average)\n",
    "    auc_roc = roc_auc_score(df_labels, dummy_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'auc_roc': auc_roc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9636\n",
      "AUC-ROC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "train_metrics_dummy = evaluate_dummy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9626\n",
      "AUC-ROC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "val_metrics_dummy = evaluate_dummy(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **accuracy** is very high (~0.96),\n",
    "an **AUC-ROC** around 0.5 indicates that this approach is equivalent to random guessing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        encoded = self.tokenizer(text)\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "\n",
    "# Dense Model\n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_units, num_layers, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        layers = []\n",
    "        input_size = embed_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(input_size, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = hidden_units\n",
    "        layers.append(nn.Linear(hidden_units, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).mean(dim=1)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, gru_units, dense_units, num_layers, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, gru_units, batch_first=True)\n",
    "        layers = []\n",
    "        input_size = gru_units\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(input_size, dense_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = dense_units\n",
    "        layers.append(nn.Linear(dense_units, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).mean(dim=1) # (batch, seq_len, embedding_dim)\n",
    "        x, _ = self.gru(x) # (batch, seq_len, hidden_units) if num_layers > 1\n",
    "        if len(x.shape) == 2:\n",
    "             # Ensure it is (batch, seq_len, hidden_units)\n",
    "            x = x.unsqueeze(1)\n",
    "        x = x[:, -1, :]  # Take the last time step\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "def build_model(config, vocab_size, num_classes):\n",
    "    model_type = config.model_type\n",
    "    if model_type == \"Dense\":\n",
    "        return DenseModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=config.embed_dim,\n",
    "            hidden_units=config.hidden_units,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            num_classes=num_classes)\n",
    "    elif model_type == \"GRU\":\n",
    "        return GRUModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=config.embed_dim,\n",
    "            gru_units=config.gru_units,\n",
    "            dense_units=config.dense_units,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            num_classes=num_classes)\n",
    "    elif model_type == \"Transformer\":\n",
    "        return TransformerModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=config.embed_dim,\n",
    "            num_heads=config.num_heads,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "\n",
    "# Compute Loss and Metrics\n",
    "def model_eval(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients during evaluation\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to convert logits to probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            # Save predictions\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append((probs >= threshold).int().cpu())\n",
    "\n",
    "    # Concatenate results\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # AUC-ROC (for multi-label, compute per class and take average)\n",
    "    auc_roc = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "\n",
    "    return avg_loss, auc_roc\n",
    "\n",
    "\n",
    "# Training function\n",
    "def model_train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # set model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        _, train_auc_roc = model_eval(model, train_loader, criterion, device)\n",
    "        val_loss, val_auc_roc = model_eval(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | AUC_ROC: {train_auc_roc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | AUC_ROC: {val_auc_roc:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_auc_roc\": train_auc_roc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_auc_roc\": val_auc_roc,\n",
    "        })\n",
    "\n",
    "    return val_auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_data.comment_text.to_list()\n",
    "train_labels = train_data.loc[:, [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "val_input = val_data.comment_text.to_list()\n",
    "val_labels = val_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "vocab = build_vocab(train_input, MIN_FREQ)\n",
    "\n",
    "# Prepare train dataset\n",
    "train_dataset = ToxicCommentsDataset(train_input, train_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)\n",
    "\n",
    "# Prepare validation dataset\n",
    "val_dataset = ToxicCommentsDataset(val_input, val_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.  And that would verify that John was a Pratt grad w/ BA/BS in Graphic Art',\n",
       " 'hi how are you  are you Mr bill \\n\\naoa \\n       hi i am waseem 4rm pakistan n whats a maining of the The International Awareness\\nPromotion Department Of\\nE.A.A.S Lottery Headquarters\\nEuro-Afro Asia Sweepstake lottery he says congratulations you have won US$250,000.00 ( (Two hundred and Fifty Thousand United States Dollars) in Cheque. pl z i have no idea tell me by this number 00923236916674 00923147007006  pless  pless pless  i shell b thank full to you',\n",
       " 'Abi 17:45, 9 February 2014',\n",
       " 'We can agree on one thing: the numbers do speak for themselves. The fact that they seem to be saying something else to you than to me, and some sources, is irrelevant.',\n",
       " 'I noticed that on the media page there are only FM radio stations.  Can someone add some AM stations?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W&B Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rimnpuii\n",
      "Sweep URL: https://wandb.ai/daniele-didino/toxic_comment_clf/sweeps/rimnpuii\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\", # \"random\" or \"grid\" or \"bayes\"\n",
    "    \"metric\": {\"name\": \"val_auc_roc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"model_type\": {\"values\": [\"Dense\", \"GRU\", \"Transformer\"]},\n",
    "\n",
    "        # Shared parameters\n",
    "        \"embed_dim\": {\"values\": [50, 100, 200]},\n",
    "        \"dropout\": {\"min\": 0.2, \"max\": 0.5},\n",
    "        \"learning_rate\": {\"min\": 1e-4, \"max\": 1e-2, \"distribution\": \"log_uniform_values\"},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"num_layers\": {\"min\": 1, \"max\": 3},\n",
    "        \"epochs\": {\"min\": 1, \"max\": 5},\n",
    "        \n",
    "        # Dense        \n",
    "        \"hidden_units\": {\"values\": [64, 128, 256]},\n",
    "        \n",
    "        # GRU\n",
    "        \"gru_units\": {\"min\": 64, \"max\": 512},\n",
    "        \"dense_units\": {\"min\": 32, \"max\": 256},\n",
    "        \n",
    "        # Transformer\n",
    "        \"num_heads\": {\"values\": [2, 4, 8]},\n",
    "        \"num_layers\": {\"values\": [2, 4, 6]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"toxic_comment_clf\")\n",
    "\n",
    "# Define the training function\n",
    "def train_sweep():\n",
    "\n",
    "    num_classes = 6  # toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config # sample hyperparameters\n",
    "        \n",
    "        # Initialize DataLoaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "        # Initialize model\n",
    "        model = build_model(config, vocab_size=len(vocab), num_classes=num_classes)\n",
    "        model.to(DEVICE)\n",
    "    \n",
    "        # Loss\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "        # Training loop\n",
    "        _ = model_train(model, train_dataloader, val_dataloader, criterion, optimizer, config[\"epochs\"], DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pu1t2df5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4645723405914543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgru_units: 104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004106593833741538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: Transformer\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_heads: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniele/Desktop/Projects/toxic_comment_clf/notebooks/wandb/run-20250227_165851-pu1t2df5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/pu1t2df5' target=\"_blank\">youthful-sweep-1</a></strong> to <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/sweeps/rimnpuii' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/sweeps/rimnpuii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/sweeps/rimnpuii' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/sweeps/rimnpuii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/pu1t2df5' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/runs/pu1t2df5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# Launch the sweep\n",
    "wandb.agent(sweep_id, function=train_sweep, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tox_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

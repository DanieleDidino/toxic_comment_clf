{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniele-didino\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(\"data\", \"processed\", \"train.csv\"))\n",
    "val_data = pd.read_csv(Path(\"data\", \"processed\", \"val.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters & wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 1 # 20\n",
    "MAX_LEN = 20\n",
    "EMBED_DIM = 50\n",
    "NUM_CLASSES = 6 # toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (5.4s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniele/Desktop/Projects/toxic_comment_clf/wandb/run-20250224_145229-xyp6zo47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/xyp6zo47' target=\"_blank\">glowing-night-1</a></strong> to <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniele-didino/toxic_comment_clf/runs/xyp6zo47' target=\"_blank\">https://wandb.ai/daniele-didino/toxic_comment_clf/runs/xyp6zo47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"toxic_comment_clf\",\n",
    "    # Track hyperparameters\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"embed_dim\": EMBED_DIM\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001, 'epochs': 1, 'batch_size': 32, 'embed_dim': 50}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tokenizer and util functions\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_vocab(texts: list[str], min_freq: int=1) -> dict:\n",
    "    token_counts = Counter()\n",
    "    for text in texts:\n",
    "        cleaned_text = clean_text(text)\n",
    "        token_counts.update(cleaned_text.split())\n",
    "    vocab = {word: idx + 2 for idx, (word, count) in enumerate(token_counts.items()) if count >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokenizer(text: str, vocab: dict, max_len: int) -> dict:\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = [vocab.get(word, 1) for word in cleaned_text.split()[:max_len]]\n",
    "    input_ids = tokens + [0] * (max_len - len(tokens))\n",
    "\n",
    "    # Check if token exceeds the len of the voceb\n",
    "    for token in input_ids:\n",
    "        if token >= len(vocab):\n",
    "            print(f\"Warning: Token index {token} out of range!\")\n",
    "    \n",
    "    return {'input_ids': torch.tensor(input_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        encoded = self.tokenizer(text)\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "\n",
    "# Model\n",
    "class ToxicityClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids).mean(dim=1)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "# Compute Loss and Metrics\n",
    "def loss_metrics(model, dataloader, criterion, device, threshold=0.5):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients during evaluation\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Apply sigmoid to convert logits to probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            # Save predictions\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append((probs >= threshold).int().cpu())\n",
    "\n",
    "    # Concatenate results\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Compute Metrics (macro-averaged for multi-label tasks)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    # AUC-ROC (for multi-label, compute per class and take average)\n",
    "    auc_roc = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc_roc\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_val(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    train_history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_loss_2\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_precision\": [],\n",
    "        \"train_recall\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"train_auc_roc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "        \"val_precision\": [],\n",
    "        \"val_recall\": [],\n",
    "        \"val_f1\": [],\n",
    "        \"val_auc_roc\": []\n",
    "    }\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # set model to training mode\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc_roc = loss_metrics(model, val_loader, criterion, device)\n",
    "        train_loss_2, train_acc, train_prec, train_rec, train_f1, train_auc_roc = loss_metrics(model, train_loader, criterion, device)\n",
    "\n",
    "        # train set\n",
    "        train_history[\"train_loss\"].append(train_loss)\n",
    "        train_history[\"train_loss_2\"].append(train_loss_2)\n",
    "        train_history[\"train_accuracy\"].append(train_acc)\n",
    "        train_history[\"train_precision\"].append(train_prec)\n",
    "        train_history[\"train_recall\"].append(train_rec)\n",
    "        train_history[\"train_f1\"].append(train_f1)\n",
    "        train_history[\"train_auc_roc\"].append(train_auc_roc)\n",
    "        # Validation set\n",
    "        train_history[\"val_loss\"].append(val_loss)\n",
    "        train_history[\"val_accuracy\"].append(val_acc)\n",
    "        train_history[\"val_precision\"].append(val_prec)\n",
    "        train_history[\"val_recall\"].append(val_rec)\n",
    "        train_history[\"val_f1\"].append(val_f1)\n",
    "        train_history[\"val_auc_roc\"].append(val_auc_roc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Train Loss: {train_loss_2:.4f} | Accuracy: {train_acc:.4f} | AUC_ROC: {train_auc_roc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} | AUC_ROC: {val_auc_roc:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_loss_2\": train_loss_2,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"train_precision\": train_prec,\n",
    "            \"train_recall\": train_rec,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"train_auc_roc\": train_auc_roc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"val_precision\": val_prec,\n",
    "            \"val_recall\": val_rec,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"val_auc_roc\": val_auc_roc,\n",
    "        })\n",
    "\n",
    "    return train_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN QUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_data.comment_text.to_list()\n",
    "train_labels = train_data.loc[:, [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n",
    "\n",
    "val_input = val_data.comment_text.to_list()\n",
    "val_labels = val_data.loc[:,  [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_input, MIN_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with masked output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss with Masking\n",
    "class HierarchicalBCELoss(nn.Module):\n",
    "    def forward(self, outputs, labels):\n",
    "        toxic_loss = nn.functional.binary_cross_entropy_with_logits(outputs[:, 0], labels[:, 0])\n",
    "\n",
    "        # Mask sub-category losses if toxic == 0 (i.e., non-toxit text)\n",
    "        mask = labels[:, 0] > 0 # Consider sub-categories only if toxic == 1 (i.e., toxic text)\n",
    "        sub_loss = nn.functional.binary_cross_entropy_with_logits(outputs[:, 1:], labels[:, 1:], reduction='none')\n",
    "        sub_loss = (sub_loss * mask.unsqueeze(1)).mean()\n",
    "\n",
    "        return toxic_loss + sub_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "dataset = ToxicCommentsDataset(train_input, train_labels, lambda text: tokenizer(text, vocab, MAX_LEN), MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m optimizer_mo \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/Projects/toxic_comment_clf/.venv_tox_clf/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/toxic_comment_clf/.venv_tox_clf/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/toxic_comment_clf/.venv_tox_clf/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_mo = ToxicityClassifier(vocab_size=len(vocab), embed_dim=EMBED_DIM, num_classes=NUM_CLASSES)\n",
    "model_mo.to(DEVICE)\n",
    "\n",
    "# Loss (custom)\n",
    "criterion_mo = HierarchicalBCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer_mo = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model_mo, dataloader, criterion_mo, optimizer_mo, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing predictions\n",
    "def postprocess_predictions(outputs):\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    toxic_pred = outputs[:, 0] >= 0.5\n",
    "\n",
    "    # Zero out sub-categories if not toxic\n",
    "    outputs[:, 1:] *= toxic_pred.unsqueeze(1)\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tox_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
